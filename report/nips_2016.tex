\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2016
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2016}

% \usepackage{nips_2016}

% to compile a camera-ready version, add the [final] option, e.g.:
\usepackage[final]{nips_2016}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amssymb}
\usepackage{mathbbol}
\usepackage{graphicx}
\usepackage{float}


\title{10-703 - Homework 2: Playing Atari With Deep Reinforcement Learning}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  Rogerio~Bonatti\ \\
  Robotics Institute\\
  Carnegie Mellon University\\
  Pittsburgh, PA 15213 \\
  \texttt{rbonatti@andrew.cmu.edu} \\
  %% examples of more authors
  \And
  Ratnesh~Madaan\ \\
  Robotics Institute\\
  Carnegie Mellon University\\
  Pittsburgh, PA 15213 \\
  \texttt{ratneshm@andrew.cmu.edu} \\
  \And
  Adithya~Murali\ \\
  Robotics Institute\\
  Carnegie Mellon University\\
  Pittsburgh, PA 15213 \\
  \texttt{amurali@andrew.cmu.edu} \\
  \And
  Alex~Spitzer\ \\
  Robotics Institute\\
  Carnegie Mellon University\\
  Pittsburgh, PA 15213 \\
  \texttt{aspitzer@andrew.cmu.edu} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}
  In this assignment we implemented various types of algorithms for control, including LQR, iLQR, Behavior Cloning and REINFORCE. 
\end{abstract}

\section{LQR}
Here are our results for the LQR algorithm. To run it, run the command "python run\_lqr py". Inside the "run lqr\_py" script the user can change the type of environment.

\subsection{[5pts] Test your LQR implementation on the TwoLinkArm-v0 environment. Record the total reward and number of steps to reach the goal. Also plot $q$, $\dot{q}$, and your control inputs $u$}

\begin{figure}[H] \label{fig:lqr_qn1}
  \centering
  \includegraphics[width=1.2\textwidth]{images/lqr_qn1}
  \caption{Plots for LQR q1}
\end{figure}

\subsection{[5pts] Test your LQR implementation on the TwoLinkArm-limited-torque-v0 environment. Record the total reward and the number of steps to reach the goal. Also plot $q$, $\dot{q}$, and your control inputs $u$. Additionally plot u clipped to the action space of this environment.}

\begin{figure}[H] \label{fig:lqr_qn2}
  \centering
  \includegraphics[width=1.2\textwidth]{images/lqr_qn2}
  \caption{Plots for LQR q2}
\end{figure}

\subsection{[5pts] Compare the performance of your controller on each of these environments}

In the unlimited torque environment the controller was able to stabilize the arm in a much shorter time than in the limited torque environment (t=40 versus t=2500). This happened because the system was able to impose extremely high torques to the arm to first move it closer to the desired position, and later de-accelerate the arm so that it remained stationary. The downside is that these very high torques are not feasible in real-life systems. 

A possible way to maintain the torques within the acceptable range in a real-life system would be to penalize high torques more in the cost function, increasing the value of components in matrix R.


\subsection{[5pts] Test your LQR implementation on the TwoLinkArm-v1 environment. Record the total reward and number of steps to reach the goal. Also plot $q$, $\dot{q}$, and your control inputs $u$}

\begin{figure}[H] \label{fig:lqr_qn4}
  \centering
  \includegraphics[width=1.2\textwidth]{images/lqr_qn4}
  \caption{Plots for LQR q4}
\end{figure}

\subsection{[5pts] Test your LQR implementation on the TwoLinkArm-limited-torque-v1 environment. Record the total reward and the number of steps to reach the goal. Also plot $q$, $\dot{q}$, and your control inputs $u$. Additionally plot u clipped to the action space of this environment.}

\begin{figure}[H] \label{fig:lqr_qn5}
  \centering
  \includegraphics[width=1.2\textwidth]{images/lqr_qn5}
  \caption{Plots for LQR q5}
\end{figure}

\subsection{[5pts] Compare the performance on these environments to the v0 versions.}

pass





\section{iLQR}
Here are our results for the iLQR algorithm. To run it, run the command "python run\_ilqr.py". Inside the "run\_ilqr.py" script the user can change the type of environment.

\subsection{[10pts] Test your iLQR implementation on the TwoLinkArm-v0 environment. Plot the total cost (intermediate cost + final cost) respect to iterations and record the total reward. Also plot $q$, $\dot{q}$, and your control inputs $u$.}

\begin{figure}[H] \label{fig:ilqr_qn1}
  \centering
  \includegraphics[width=1.2\textwidth]{images/ilqr_qn1}
  \caption{Plots for iLQR q1}
\end{figure}

\subsection{[10pts] Test your iLQR implementation on the TwoLinkArm-v1 environment. Plot the total cost (intermediate cost + final cost) respect to iterations and record the total reward. Also plot $q$, $\dot{q}$, and your control inputs $u$.}

\begin{figure}[H] \label{fig:ilqr_qn2}
  \centering
  \includegraphics[width=1.2\textwidth]{images/ilqr_qn2}
  \caption{Plots for iLQR q2}
\end{figure}

\subsection{[5pts] Discuss the comparison between iLQR and LQR algorithm, which one is better and why?}

pass

\subsection{[5pts EXTRA CREDIT] iLQR is not as fast as we want. Try to improve the convergence in any way you can figure out. Potential directions include changing cost function and use better optimization procedures.}

pass




\section{Behavior cloning}
Here are our results for the behavior cloning algorithm. 

\subsection{[10pts] Use each of your datasets to train a cloned behavior using the dataset as a supervised learning problem. Record the final loss and accuracy of your model after training for at least 50 epochs. Make sure you include any hyper parameters in your report}

pass

\subsection{[[10pts] Evaluate each of your cloned models on the CartPole-v0 model using the test cloned policy method. How does the amount of training data affect the cloned policy?}

pass

\subsection{[10pts] Evaluate each of your cloned models on the CartPole-v0 domain wrapped with the wrap cartpole function. Also evaluate the expert policy. How does your cloned behavior compare with respect to the expert policies and each other.}

pass



\section{REINFORCE}
Here are our results for the REINFORCE algorithm. 

\subsection{[10pts] Train REINFORCE on CartPole-v0 until convergence. Show your learning curves for the agent. In other words, every k episodes, freeze the current cloned policy and run 100 test episodes. Average the total reward and track the min and max reward. Plot the total reward on the y-axis with min/max values as error-bars vs the number of training episodes.}

pass

\subsection{[10pts] How does this policy compare to our provided expert and your cloned models?}

pass


% \section*{References}

\small
\medskip
\bibliographystyle{plain}
\bibliography{bibliography}


\end{document}
